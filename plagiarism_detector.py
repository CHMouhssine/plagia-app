# -*- coding: utf-8 -*-
"""
D√âTECTEUR DE PLAGIAT COMPLET AVEC INTERFACE STREAMLIT
Installation: pip install streamlit scikit-learn plotly pandas numpy
Ex√©cution: streamlit run app.py
"""
import zipfile
from xml.dom.minidom import Document

import PyPDF2
import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re
from collections import Counter
import warnings
warnings.filterwarnings('ignore')
from sentence_transformers import SentenceTransformer

# ============================================================================
# CLASSE PRINCIPALE - D√âTECTEUR DE PLAGIAT
# ============================================================================
import joblib
st.set_page_config(
    page_title="D√©tecteur de Plagiat IA",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Charger une seule fois
if 'svm_model' not in st.session_state:
    st.session_state.svm_model = joblib.load("svm_model.joblib")

if 'scaler' not in st.session_state:
    st.session_state.scaler = joblib.load("scaler.joblib")


if 'sbert_model' not in st.session_state:
    st.session_state.sbert_model = SentenceTransformer('all-MiniLM-L6-v2')  # same as training




def detect_plagiarism_svm(documents, model, scaler, threshold):
        # Extraire les contenus
        texts = [doc['processed_content'] for doc in documents]

        # Re-vectoriser avec le m√™me scaler que lors de l'entra√Ænement
        X = scaler.transform(texts)

        similarities = []
        n_docs = len(documents)
        for i in range(n_docs):
            for j in range(i + 1, n_docs):
                # Cr√©er la diff√©rence vectorielle ou concat√©ner les deux
                pair_features = np.abs(X[i] - X[j])  # ou np.concatenate([X[i], X[j]])
                pair_features = pair_features.reshape(1, -1)

                pred = model.predict(pair_features)[0]
                prob = getattr(model, "predict_proba", lambda x: [[0.0, 1.0]])(pair_features)[0][1]

                similarities.append({
                    'doc1_id': i,
                    'doc2_id': j,
                    'doc1_title': documents[i]['title'],
                    'doc2_title': documents[j]['title'],
                    'similarity': prob,
                    'is_plagiarism': pred == 1,
                    'percentage': prob * 100
                })

        return sorted(similarities, key=lambda x: x['similarity'], reverse=True)
class PlagiarismDetector:
    """
    D√©tecteur de plagiat utilisant TF-IDF et similarit√© cosinus
    """

    def __init__(self, threshold=0.7, language='french'):
        """
        Initialise le d√©tecteur de plagiat

        Args:
            threshold (float): Seuil de d√©tection (0-1)
            language (str): Langue pour les stop words
        """
        self.threshold = threshold
        self.language = language
        self.documents = []
        self.tfidf_matrix = None
        self.vectorizer = None
        self.similarities = []
        self.vocabulary = []

    def preprocess_text(self, text):
        """
        Pr√©processe le texte pour l'analyse

        Args:
            text (str): Texte √† pr√©processer

        Returns:
            str: Texte pr√©process√©
        """
        # Conversion en minuscules
        text = text.lower()

        # Suppression des caract√®res sp√©ciaux (garde les accents fran√ßais)
        text = re.sub(r'[^\w\s√†√¢√§√©√®√™√´√Ø√Æ√¥√∂√π√ª√º√ø√ß]', ' ', text)

        # Suppression des espaces multiples
        text = re.sub(r'\s+', ' ', text).strip()

        return text

    def add_document(self, title, content):
        """
        Ajoute un document √† analyser

        Args:
            title (str): Titre du document
            content (str): Contenu du document
        """
        processed_content = self.preprocess_text(content)
        self.documents.append({
            'id': len(self.documents) + 1,
            'title': title,
            'content': content,
            'processed_content': processed_content,
            'word_count': len(processed_content.split())
        })

    def calculate_tfidf(self):
        """
        Calcule la matrice TF-IDF pour tous les documents
        """
        if len(self.documents) < 2:
            raise ValueError("Au moins 2 documents sont n√©cessaires pour l'analyse")

        # Extraction du contenu pr√©process√©
        texts = [doc['processed_content'] for doc in self.documents]

        # Configuration du vectoriseur TF-IDF
        self.vectorizer = TfidfVectorizer(
            max_features=5000,  # Limite le vocabulaire
            ngram_range=(1, 2), # Unigrams et bigrams
            min_df=1,           # Minimum document frequency
            stop_words=None     # Pas de stop words pour plus de pr√©cision
        )

        # Calcul de la matrice TF-IDF
        self.tfidf_matrix = self.vectorizer.fit_transform(texts)

        # Extraction du vocabulaire
        self.vocabulary = self.vectorizer.get_feature_names_out()

    def calculate_similarities(self):
        """
        Calcule les similarit√©s entre toutes les paires de documents
        """
        if self.tfidf_matrix is None:
            self.calculate_tfidf()

        # Calcul de la matrice de similarit√© cosinus
        similarity_matrix = cosine_similarity(self.tfidf_matrix)

        # Extraction des paires et leurs similarit√©s
        self.similarities = []
        n_docs = len(self.documents)

        for i in range(n_docs):
            for j in range(i + 1, n_docs):
                similarity_score = similarity_matrix[i][j]

                self.similarities.append({
                    'doc1_id': i,
                    'doc2_id': j,
                    'doc1_title': self.documents[i]['title'],
                    'doc2_title': self.documents[j]['title'],
                    'similarity': similarity_score,
                    'is_plagiarism': similarity_score > self.threshold,
                    'percentage': similarity_score * 100
                })

        # Tri par similarit√© d√©croissante
        self.similarities.sort(key=lambda x: x['similarity'], reverse=True)

    def detect_plagiarism(self):
        """
        Lance la d√©tection compl√®te de plagiat

        Returns:
            list: Liste des r√©sultats de d√©tection
        """
        self.calculate_similarities()
        return self.similarities

    def get_detailed_results(self):
        """
        Retourne les r√©sultats d√©taill√©s sous forme de DataFrame

        Returns:
            pd.DataFrame: R√©sultats d√©taill√©s
        """
        if not self.similarities:
            self.detect_plagiarism()

        df = pd.DataFrame(self.similarities)
        df['similarity_percentage'] = df['similarity'] * 100

        return df[['doc1_title', 'doc2_title', 'similarity_percentage', 'is_plagiarism']]



    def get_vocabulary_stats(self, top_n=20):
        """
        Retourne les statistiques sur le vocabulaire

        Args:
            top_n (int): Nombre de termes les plus fr√©quents √† retourner

        Returns:
            dict: Statistiques du vocabulaire
        """
        if self.tfidf_matrix is None:
            self.calculate_tfidf()

        # Calcul des scores TF-IDF moyens pour chaque terme
        mean_scores = np.array(self.tfidf_matrix.mean(axis=0)).flatten()
        vocab_scores = list(zip(self.vocabulary, mean_scores))
        vocab_scores.sort(key=lambda x: x[1], reverse=True)

        return {
            'total_terms': len(self.vocabulary),
            'top_terms': vocab_scores[:top_n],
            'matrix_shape': self.tfidf_matrix.shape
        }

# ============================================================================
# INTERFACE STREAMLIT
# ============================================================================

# Configuration de la page

# Titre principal avec style
st.markdown("""
<div style="text-align: center; padding: 2rem 0;">
    <h1 style="color: #2E86C1; font-size: 3rem; margin-bottom: 0.5rem;">
         D√©tecteur de Plagiat IA
    </h1>
    <p style="color: #5D6D7E; font-size: 1.2rem;">
        Analyse de similarit√© textuelle bas√©e sur TF-IDF et similarit√© cosinus
    </p>
</div>
""", unsafe_allow_html=True)

# Initialisation de l'√©tat de session
if 'detector' not in st.session_state:
    st.session_state.detector = PlagiarismDetector()
if 'documents' not in st.session_state:
    st.session_state.documents = []
if 'analysis_done' not in st.session_state:
    st.session_state.analysis_done = False

# Sidebar pour la configuration
st.sidebar.header("‚öôÔ∏è Configuration")

# Seuil de d√©tection
threshold = st.sidebar.slider(
    "Seuil de d√©tection (%)",
    min_value=30,
    max_value=90,
    value=70,
    help="Pourcentage de similarit√© au-dessus duquel un plagiat est d√©tect√©"
)
st.session_state.detector.threshold = threshold / 100

# Section d'ajout de documents
st.sidebar.header("üß† Choix du Mod√®le de Similarit√©")

selected_model = st.sidebar.selectbox(
    "Mod√®le de Similarit√©",
    ["Cosine Similarity (TF-IDF)", "SBERT", "LSTM", "SVM"],
    help="Choisissez la m√©thode utilis√©e pour comparer les documents"
)

# Stocker le mod√®le choisi dans la session
st.session_state["selected_model"] = selected_model

st.sidebar.header("üìÑ Gestion des Documents")

with st.sidebar.expander("‚ûï Ajouter un Document", expanded=True):
    with st.form("add_document"):
        doc_title = st.text_input("Titre du document")
        doc_content = st.text_area("Contenu du document", height=150)

        if st.form_submit_button("Ajouter Document"):
            if doc_title and doc_content:
                st.session_state.detector.add_document(doc_title, doc_content)
                st.session_state.documents.append({
                    'title': doc_title,
                    'content': doc_content,
                    'word_count': len(doc_content.split())
                })
                st.session_state.analysis_done = False
                st.success(f"Document '{doc_title}' ajout√©!")
            else:
                st.error("Veuillez remplir le titre et le contenu")

# Charger des exemples
    st.sidebar.button("üìö Charger des Exemples")
    uploaded_files = st.sidebar.file_uploader(
        "Uploader un ou plusieurs fichiers (TXT, PDF, DOCX ou ZIP)",
        type=['txt', 'pdf', 'docx', 'zip'],
        accept_multiple_files=True
    )

    if uploaded_files:
        st.session_state.detector = PlagiarismDetector(threshold / 100)
        st.session_state.documents = []


        def extract_text(file):
            file_type = file.name.split('.')[-1].lower()
            if file_type == 'txt':
                return file.read().decode('utf-8', errors='ignore')
            elif file_type == 'pdf':
                reader = PyPDF2.PdfReader(file)
                return '\n'.join(page.extract_text() or '' for page in reader.pages)
            elif file_type == 'docx':
                doc = Document(file)
                return '\n'.join(paragraph.text for paragraph in doc.paragraphs)
            return ''


        for file in uploaded_files:
            if file.name.endswith('.zip'):
                with zipfile.ZipFile(file, 'r') as z:
                    for inner_file_name in z.namelist():
                        if inner_file_name.endswith(('.txt', '.pdf', '.docx')):
                            with z.open(inner_file_name) as inner_file:
                                content = extract_text(inner_file)
                                st.session_state.detector.add_document(inner_file_name, content)
                                st.session_state.documents.append({
                                    'title': inner_file_name,
                                    'content': content,
                                    'word_count': len(content.split())
                                })
            else:
                content = extract_text(file)
                st.session_state.detector.add_document(file.name, content)
                st.session_state.documents.append({
                    'title': file.name,
                    'content': content,
                    'word_count': len(content.split())
                })

        st.sidebar.success("Fichiers import√©s avec succ√®s!")
        st.session_state.analysis_done = False

# Bouton de suppression des documents
if st.sidebar.button("üóëÔ∏è Supprimer tous les documents"):
    st.session_state.detector = PlagiarismDetector(threshold/100)
    st.session_state.documents = []
    st.session_state.analysis_done = False
    st.sidebar.success("Documents supprim√©s!")

# Interface principale
col1, col2 = st.columns([1, 1])





with col1:
    st.header("üìã Documents Charg√©s")

    if st.session_state.documents:
        to_remove = None

        for i, doc in enumerate(st.session_state.documents):
            col1_doc, col2_doc = st.columns([0.9, 0.1])
            with col1_doc:
                with st.expander(f"üìÑ {doc['title']} ({doc['word_count']} mots)"):
                    st.text_area("Contenu", value=doc['content'][:500], height=100, disabled=True, key=f"doc_content_{i}")
            with col2_doc:
                if st.button("‚ùå", key=f"remove_{i}"):
                    to_remove = i

        # Supprimer le document imm√©diatement
        if to_remove is not None:
            del st.session_state.documents[to_remove]
            del st.session_state.detector.documents[to_remove]
            st.session_state.analysis_done = False  # R√©initialiser √©tat d'analyse
            st.success("Document supprim√© avec succ√®s.")
            st.rerun()
 # ‚úÖ Recharge imm√©diat

        # Bouton d'analyse
        if st.button("üîç Analyser les Documents", type="primary", use_container_width=True):
            if len(st.session_state.documents) < 2:
                st.error("Au moins 2 documents sont n√©cessaires pour l'analyse")
            else:
                with st.spinner("Analyse en cours..."):
                    try:
                        model = st.session_state.get("selected_model", "Cosine Similarity (TF-IDF)")
                        if model == "Cosine Similarity (TF-IDF)":
                            results = st.session_state.detector.detect_plagiarism()
                        elif model == "SBERT":
                            st.warning("SBERT pas encore impl√©ment√©.")
                        elif model == "LSTM":
                            st.warning("LSTM pas encore impl√©ment√©.")
                        elif model == "SVM":

                            results = detect_plagiarism_svm(
                                documents=st.session_state.detector.documents,
                                model=st.session_state.svm_model,
                                scaler=st.session_state.scaler,
                                threshold=st.session_state.detector.threshold
                            )

                            st.session_state.detector.similarities = results

                            if not results:
                                st.warning("Aucune similarit√© d√©tect√©e par le mod√®le SVM.")
                            else:
                                st.write("üîç R√©sultats SVM bruts :")
                                st.write(results)
                        st.session_state.analysis_done = True
                        st.success("Analyse termin√©e!")
                    except Exception as e:
                        st.error(f"Erreur lors de l'analyse: {str(e)}")
    else:
        st.info("Aucun document charg√©. Utilisez la barre lat√©rale pour ajouter des documents.")

with col2:
    st.header("üìä R√©sultats de l'Analyse")

    if st.session_state.analysis_done and st.session_state.detector.similarities:
        # Tableau des r√©sultats
        df_results = st.session_state.detector.get_detailed_results()
        st.subheader("R√©sultats D√©taill√©s")

        # Formatage du DataFrame pour l'affichage
        df_display = df_results.copy()
        df_display['similarity_percentage'] = df_display['similarity_percentage'].round(2)
        df_display['is_plagiarism'] = df_display['is_plagiarism'].map({True: '‚ö†Ô∏è Plagiat', False: '‚úÖ Original'})
        df_display.columns = ['Document 1', 'Document 2', 'Similarit√© (%)', 'Statut']

        st.dataframe(df_display, use_container_width=True)

        # Statistiques rapides
        plagiarism_count = sum(1 for s in st.session_state.detector.similarities if s['is_plagiarism'])
        total_comparisons = len(st.session_state.detector.similarities)

        col_stat1, col_stat2, col_stat3 = st.columns(3)
        with col_stat1:
            st.metric("Documents", len(st.session_state.documents))
        with col_stat2:
            st.metric("Comparaisons", total_comparisons)
        with col_stat3:
            st.metric("Cas de plagiat", plagiarism_count)

        # Graphique des similarit√©s
        st.subheader("Visualisation des Similarit√©s")

        similarities_data = []
        colors = []

        for s in st.session_state.detector.similarities:
            comparison = f"{s['doc1_title'][:15]}... vs {s['doc2_title'][:15]}..."
            similarities_data.append({
                'Comparaison': comparison,
                'Similarit√© (%)': s['percentage'],
                'Type': 'Plagiat D√©tect√©' if s['is_plagiarism'] else 'Document Original'
            })

        df_viz = pd.DataFrame(similarities_data)

        fig = px.bar(df_viz,
                    x='Comparaison',
                    y='Similarit√© (%)',
                    color='Type',
                    color_discrete_map={
                        'Plagiat D√©tect√©': '#FF6B6B',
                        'Document Original': '#4ECDC4'
                    },
                    title='Scores de Similarit√© par Paire de Documents')

        # Ligne de seuil
        fig.add_hline(y=threshold,
                     line_dash="dash",
                     line_color="red",
                     annotation_text=f"Seuil de plagiat ({threshold}%)")

        fig.update_layout(xaxis_tickangle=-45, height=500)
        st.plotly_chart(fig, use_container_width=True)

        # Matrice de similarit√©
        if len(st.session_state.documents) <= 10:  # Limite pour la lisibilit√©
            st.subheader("Matrice de Similarit√©")

            # Calcul de la matrice de similarit√© compl√®te
            similarity_matrix = cosine_similarity(st.session_state.detector.tfidf_matrix)

            # Cr√©ation des labels
            labels = [doc['title'][:20] + '...' if len(doc['title']) > 20
                     else doc['title'] for doc in st.session_state.documents]

            # Heatmap avec Plotly
            fig_heatmap = px.imshow(similarity_matrix,
                                   x=labels,
                                   y=labels,
                                   color_continuous_scale='RdYlBu_r',
                                   aspect='auto',
                                   title='Matrice de Similarit√© entre Documents')

            fig_heatmap.update_layout(height=500)
            st.plotly_chart(fig_heatmap, use_container_width=True)

    elif st.session_state.documents and not st.session_state.analysis_done:
        st.info("Cliquez sur 'Analyser les Documents' pour voir les r√©sultats.")
    else:
        st.info("Aucune analyse disponible. Ajoutez des documents et lancez l'analyse.")

# Section d'aide
with st.expander("‚ÑπÔ∏è Comment utiliser ce d√©tecteur de plagiat"):
    st.markdown("""
    ### Instructions d'utilisation:
    
    1. **Ajouter des documents**: Utilisez la barre lat√©rale pour ajouter vos documents un par un
    2. **Ou charger des exemples**: Cliquez sur "Charger des Exemples" pour tester avec des donn√©es de d√©monstration
    3. **Configurer le seuil**: Ajustez le seuil de d√©tection selon vos besoins (70% par d√©faut)
    4. **Analyser**: Cliquez sur "Analyser les Documents" pour d√©tecter les similarit√©s
    5. **Interpr√©ter les r√©sultats**: 
       - Rouge = Plagiat d√©tect√© (au-dessus du seuil)
       - Vert = Document original (en-dessous du seuil)
    
    ### M√©thode utilis√©e:
    - **TF-IDF**: Calcul de l'importance des mots dans chaque document
    - **Similarit√© cosinus**: Mesure de l'angle entre les vecteurs de documents
    - **Seuil personnalisable**: Vous d√©finissez √† partir de quel pourcentage consid√©rer un plagiat
    
    ### Limites:
    - D√©tection bas√©e sur la similarit√© lexicale
    - Ne d√©tecte pas la paraphrase sophistiqu√©e
    - Sensible √† la longueur des documents
    """)

# Footer
st.markdown("---")
st.markdown(
    "<div style='text-align: center; color: #7F8C8D;'>"
    "D√©tecteur de Plagiat IA - Bas√© sur TF-IDF et Similarit√© Cosinus"
    "</div>",
    unsafe_allow_html=True
)
