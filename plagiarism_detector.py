# -*- coding: utf-8 -*-
"""
D√âTECTEUR DE PLAGIAT COMPLET AVEC INTERFACE STREAMLIT
Installation: pip install streamlit scikit-learn plotly pandas numpy tensorflow
Ex√©cution: streamlit run app.py
"""
import zipfile
from xml.dom.minidom import Document

import PyPDF2
import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re
from collections import Counter
import warnings
warnings.filterwarnings('ignore')
from sentence_transformers import SentenceTransformer
import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences

# ============================================================================
# CLASSE PRINCIPALE - D√âTECTEUR DE PLAGIAT
# ============================================================================
import joblib
st.set_page_config(
    page_title="D√©tecteur de Plagiat IA",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Charger les mod√®les une seule fois
if 'svm_model' not in st.session_state:
    try:
        st.session_state.svm_model = joblib.load("svm_model2.joblib")
    except FileNotFoundError:
        st.session_state.svm_model = None
        st.warning("Mod√®le SVM non trouv√© (svm_model2.joblib)")

if 'scaler' not in st.session_state:
    try:
        st.session_state.scaler = joblib.load("scaler2.joblib")
    except FileNotFoundError:
        st.session_state.scaler = None
        st.warning("Scaler non trouv√© (scaler2.joblib)")

if 'sbert_model' not in st.session_state:
    try:
        st.session_state.sbert_model = SentenceTransformer('all-MiniLM-L6-v2')
    except Exception as e:
        st.session_state.sbert_model = None
        st.warning(f"Mod√®le SBERT non disponible: {e}")

# Charger le mod√®le LSTM et le tokenizer
if 'lstm_model' not in st.session_state:
    try:
        st.session_state.lstm_model = tf.keras.models.load_model("lstm_model2.h5")
    except FileNotFoundError:
        st.session_state.lstm_model = None
        st.warning("Mod√®le LSTM non trouv√© (lstm_model2.h5)")

if 'tokenizer' not in st.session_state:
    try:
        st.session_state.tokenizer = joblib.load("tokenizer2.joblib")
    except FileNotFoundError:
        st.session_state.tokenizer = None
        st.warning("Tokenizer non trouv√© (tokenizer2.joblib)")

def create_similarity_features(emb1, emb2):
    cosine_sim = np.sum(emb1 * emb2, axis=1).reshape(-1, 1)
    euclidean_dist = np.linalg.norm(emb1 - emb2, axis=1).reshape(-1, 1)
    manhattan_dist = np.sum(np.abs(emb1 - emb2), axis=1).reshape(-1, 1)

    abs_diff = np.abs(emb1 - emb2)
    variances = np.var(abs_diff, axis=0)
    top50_idx = np.argsort(variances)[-50:]  # ‚úÖ comme √† l'entra√Ænement

    prod_top = (emb1 * emb2)[:, top50_idx]
    diff_top = abs_diff[:, top50_idx]

    stats = np.concatenate([
        np.max(abs_diff, axis=1).reshape(-1, 1),
        np.min(abs_diff, axis=1).reshape(-1, 1),
        np.mean(abs_diff, axis=1).reshape(-1, 1),
        np.std(abs_diff, axis=1).reshape(-1, 1)
    ], axis=1)

    return np.concatenate([
        cosine_sim, euclidean_dist, manhattan_dist,
        diff_top, prod_top, stats
    ], axis=1)

def detect_plagiarism_svm(documents, model, scaler, threshold):
    if model is None or scaler is None or st.session_state.sbert_model is None:
        st.error("Mod√®le SVM, scaler ou SBERT non disponible")
        return []
    
    texts = [doc['processed_content'] for doc in documents]
    titles = [doc['title'] for doc in documents]

    # Embeddings SBERT
    emb = st.session_state.sbert_model.encode(texts, normalize_embeddings=True)

    similarities = []
    n_docs = len(texts)
    for i in range(n_docs):
        for j in range(i + 1, n_docs):
            emb1 = emb[i].reshape(1, -1)
            emb2 = emb[j].reshape(1, -1)

            # Cr√©er les features avanc√©es
            feats = create_similarity_features(emb1, emb2)
            feats_scaled = scaler.transform(feats)

            pred = model.predict(feats_scaled)[0]
            prob = model.predict_proba(feats_scaled)[0][1]

            similarities.append({
                'doc1_id': i,
                'doc2_id': j,
                'doc1_title': titles[i],
                'doc2_title': titles[j],
                'similarity': prob,
                'is_plagiarism': pred == 1,
                'percentage': prob * 100
            })

    return sorted(similarities, key=lambda x: x['similarity'], reverse=True)

def detect_plagiarism_lstm(documents, model, tokenizer, threshold, max_length=40):
    """
    D√©tection de plagiat utilisant le mod√®le LSTM
    
    Args:
        documents: Liste des documents
        model: Mod√®le LSTM charg√©
        tokenizer: Tokenizer pour la vectorisation
        threshold: Seuil de d√©tection
        max_length: Longueur maximale des s√©quences (doit correspondre √† l'entra√Ænement)
    
    Returns:
        Liste des r√©sultats de similarit√©
    """
    if model is None or tokenizer is None:
        st.error("Mod√®le LSTM ou tokenizer non disponible")
        return []
    
    texts = [doc['processed_content'] for doc in documents]
    titles = [doc['title'] for doc in documents]

    similarities = []
    n_docs = len(texts)
    
    for i in range(n_docs):
        for j in range(i + 1, n_docs):
            # Pr√©parer les textes pour le mod√®le LSTM
            text1 = texts[i]
            text2 = texts[j]
            
            # Tokenisation et padding - S√âPAR√âMENT comme dans l'entra√Ænement
            seq1 = tokenizer.texts_to_sequences([text1])
            seq2 = tokenizer.texts_to_sequences([text2])
            
            # Padding des s√©quences avec les m√™mes param√®tres qu'√† l'entra√Ænement
            padded_seq1 = pad_sequences(seq1, maxlen=max_length, padding='post', truncating='post')
            padded_seq2 = pad_sequences(seq2, maxlen=max_length, padding='post', truncating='post')
            
            # Pr√©parer l'entr√©e pour le mod√®le 
            # Le mod√®le attend probablement deux entr√©es s√©par√©es [X1, X2]
            try:
                # Si votre mod√®le utilise deux entr√©es s√©par√©es (architecture siamoise)
                prediction = model.predict([padded_seq1, padded_seq2], verbose=0)[0][0]
                
                # OU si votre mod√®le attend une seule entr√©e concat√©n√©e:
                # model_input = np.concatenate([padded_seq1, padded_seq2], axis=1)
                # prediction = model.predict(model_input, verbose=0)[0][0]
                
                similarities.append({
                    'doc1_id': i,
                    'doc2_id': j,
                    'doc1_title': titles[i],
                    'doc2_title': titles[j],
                    'similarity': prediction,
                    'is_plagiarism': prediction > threshold,
                    'percentage': prediction * 100
                })
            except Exception as e:
                st.error(f"Erreur lors de la pr√©diction LSTM: {e}")
                continue

    return sorted(similarities, key=lambda x: x['similarity'], reverse=True)
def detect_plagiarism_sbert(documents, threshold):
    """
    D√©tection de plagiat utilisant SBERT
    """
    if st.session_state.sbert_model is None:
        st.error("Mod√®le SBERT non disponible")
        return []
    
    texts = [doc['processed_content'] for doc in documents]
    titles = [doc['title'] for doc in documents]

    # Embeddings SBERT
    embeddings = st.session_state.sbert_model.encode(texts, normalize_embeddings=True)
    
    similarities = []
    n_docs = len(texts)
    
    for i in range(n_docs):
        for j in range(i + 1, n_docs):
            # Calcul de la similarit√© cosinus
            similarity_score = np.dot(embeddings[i], embeddings[j])
            
            similarities.append({
                'doc1_id': i,
                'doc2_id': j,
                'doc1_title': titles[i],
                'doc2_title': titles[j],
                'similarity': similarity_score,
                'is_plagiarism': similarity_score > threshold,
                'percentage': similarity_score * 100
            })

    return sorted(similarities, key=lambda x: x['similarity'], reverse=True)

class PlagiarismDetector:
    """
    D√©tecteur de plagiat utilisant TF-IDF et similarit√© cosinus
    """

    def __init__(self, threshold=0.7, language='french'):
        """
        Initialise le d√©tecteur de plagiat

        Args:
            threshold (float): Seuil de d√©tection (0-1)
            language (str): Langue pour les stop words
        """
        self.threshold = threshold
        self.language = language
        self.documents = []
        self.tfidf_matrix = None
        self.vectorizer = None
        self.similarities = []
        self.vocabulary = []

    def preprocess_text(self, text):
        """
        Pr√©processe le texte pour l'analyse

        Args:
            text (str): Texte √† pr√©processer

        Returns:
            str: Texte pr√©process√©
        """
        # Conversion en minuscules
        text = text.lower()

        # Suppression des caract√®res sp√©ciaux (garde les accents fran√ßais)
        text = re.sub(r'[^\w\s√†√¢√§√©√®√™√´√Ø√Æ√¥√∂√π√ª√º√ø√ß]', ' ', text)

        # Suppression des espaces multiples
        text = re.sub(r'\s+', ' ', text).strip()

        return text

    def add_document(self, title, content):
        """
        Ajoute un document √† analyser

        Args:
            title (str): Titre du document
            content (str): Contenu du document
        """
        processed_content = self.preprocess_text(content)
        self.documents.append({
            'id': len(self.documents) + 1,
            'title': title,
            'content': content,
            'processed_content': processed_content,
            'word_count': len(processed_content.split())
        })

    def calculate_tfidf(self):
        """
        Calcule la matrice TF-IDF pour tous les documents
        """
        if len(self.documents) < 2:
            raise ValueError("Au moins 2 documents sont n√©cessaires pour l'analyse")

        # Extraction du contenu pr√©process√©
        texts = [doc['processed_content'] for doc in self.documents]

        # Configuration du vectoriseur TF-IDF
        self.vectorizer = TfidfVectorizer(
            max_features=5000,  # Limite le vocabulaire
            ngram_range=(1, 2), # Unigrams et bigrams
            min_df=1,           # Minimum document frequency
            stop_words=None     # Pas de stop words pour plus de pr√©cision
        )

        # Calcul de la matrice TF-IDF
        self.tfidf_matrix = self.vectorizer.fit_transform(texts)

        # Extraction du vocabulaire
        self.vocabulary = self.vectorizer.get_feature_names_out()

    def calculate_similarities(self):
        """
        Calcule les similarit√©s entre toutes les paires de documents
        """
        if self.tfidf_matrix is None:
            self.calculate_tfidf()

        # Calcul de la matrice de similarit√© cosinus
        similarity_matrix = cosine_similarity(self.tfidf_matrix)

        # Extraction des paires et leurs similarit√©s
        self.similarities = []
        n_docs = len(self.documents)

        for i in range(n_docs):
            for j in range(i + 1, n_docs):
                similarity_score = similarity_matrix[i][j]

                self.similarities.append({
                    'doc1_id': i,
                    'doc2_id': j,
                    'doc1_title': self.documents[i]['title'],
                    'doc2_title': self.documents[j]['title'],
                    'similarity': similarity_score,
                    'is_plagiarism': similarity_score > self.threshold,
                    'percentage': similarity_score * 100
                })

        # Tri par similarit√© d√©croissante
        self.similarities.sort(key=lambda x: x['similarity'], reverse=True)

    def detect_plagiarism(self):
        """
        Lance la d√©tection compl√®te de plagiat

        Returns:
            list: Liste des r√©sultats de d√©tection
        """
        self.calculate_similarities()
        return self.similarities

    def get_detailed_results(self):
        """
        Retourne les r√©sultats d√©taill√©s sous forme de DataFrame

        Returns:
            pd.DataFrame: R√©sultats d√©taill√©s
        """
        if not self.similarities:
            self.detect_plagiarism()

        df = pd.DataFrame(self.similarities)
        df['similarity_percentage'] = df['similarity'] * 100

        return df[['doc1_title', 'doc2_title', 'similarity_percentage', 'is_plagiarism']]

    def get_vocabulary_stats(self, top_n=20):
        """
        Retourne les statistiques sur le vocabulaire

        Args:
            top_n (int): Nombre de termes les plus fr√©quents √† retourner

        Returns:
            dict: Statistiques du vocabulaire
        """
        if self.tfidf_matrix is None:
            self.calculate_tfidf()

        # Calcul des scores TF-IDF moyens pour chaque terme
        mean_scores = np.array(self.tfidf_matrix.mean(axis=0)).flatten()
        vocab_scores = list(zip(self.vocabulary, mean_scores))
        vocab_scores.sort(key=lambda x: x[1], reverse=True)

        return {
            'total_terms': len(self.vocabulary),
            'top_terms': vocab_scores[:top_n],
            'matrix_shape': self.tfidf_matrix.shape
        }

# ============================================================================
# INTERFACE STREAMLIT
# ============================================================================

# Titre principal avec style
st.markdown("""
<div style="text-align: center; padding: 2rem 0;">
    <h1 style="color: #2E86C1; font-size: 3rem; margin-bottom: 0.5rem;">
         D√©tecteur de Plagiat IA
    </h1>
    <p style="color: #5D6D7E; font-size: 1.2rem;">
        Analyse de similarit√© textuelle avec multiple mod√®les IA
    </p>
</div>
""", unsafe_allow_html=True)

# Initialisation de l'√©tat de session
if 'detector' not in st.session_state:
    st.session_state.detector = PlagiarismDetector()
if 'documents' not in st.session_state:
    st.session_state.documents = []
if 'analysis_done' not in st.session_state:
    st.session_state.analysis_done = False

# Sidebar pour la configuration
st.sidebar.header("‚öôÔ∏è Configuration")

# Seuil de d√©tection
threshold = st.sidebar.slider(
    "Seuil de d√©tection (%)",
    min_value=30,
    max_value=90,
    value=70,
    help="Pourcentage de similarit√© au-dessus duquel un plagiat est d√©tect√©"
)
st.session_state.detector.threshold = threshold / 100

# Section d'ajout de documents
st.sidebar.header("üß† Choix du Mod√®le de Similarit√©")

# V√©rifier la disponibilit√© des mod√®les
available_models = ["Cosine Similarity (TF-IDF)"]
if st.session_state.sbert_model is not None:
    available_models.append("SBERT")
if st.session_state.lstm_model is not None and st.session_state.tokenizer is not None:
    available_models.append("LSTM")
if st.session_state.svm_model is not None and st.session_state.scaler is not None:
    available_models.append("SVM")

selected_model = st.sidebar.selectbox(
    "Mod√®le de Similarit√©",
    available_models,
    help="Choisissez la m√©thode utilis√©e pour comparer les documents"
)

# Stocker le mod√®le choisi dans la session
st.session_state["selected_model"] = selected_model

# Param√®tres sp√©cifiques au mod√®le LSTM
# Param√®tres sp√©cifiques au mod√®le LSTM
if selected_model == "LSTM":
    max_sequence_length = st.sidebar.slider(
        "Longueur maximale des s√©quences (LSTM)",
        min_value=20,
        max_value=100,
        value=40,  # Chang√© de 100 √† 40 pour correspondre √† votre entra√Ænement
        help="Longueur maximale des s√©quences pour le mod√®le LSTM (doit correspondre √† l'entra√Ænement)"
    )
    st.session_state["max_sequence_length"] = max_sequence_length

st.sidebar.header("üìÑ Gestion des Documents")

with st.sidebar.expander("‚ûï Ajouter un Document", expanded=True):
    with st.form("add_document"):
        doc_title = st.text_input("Titre du document")
        doc_content = st.text_area("Contenu du document", height=150)

        if st.form_submit_button("Ajouter Document"):
            if doc_title and doc_content:
                st.session_state.detector.add_document(doc_title, doc_content)
                st.session_state.documents.append({
                    'title': doc_title,
                    'content': doc_content,
                    'word_count': len(doc_content.split())
                })
                st.session_state.analysis_done = False
                st.success(f"Document '{doc_title}' ajout√©!")
            else:
                st.error("Veuillez remplir le titre et le contenu")

# Charger des exemples
st.sidebar.button("üìö Charger des Exemples")
uploaded_files = st.sidebar.file_uploader(
    "Uploader un ou plusieurs fichiers (TXT, PDF, DOCX ou ZIP)",
    type=['txt', 'pdf', 'docx', 'zip'],
    accept_multiple_files=True
)

if uploaded_files:
    st.session_state.detector = PlagiarismDetector(threshold / 100)
    st.session_state.documents = []

    def extract_text(file):
        file_type = file.name.split('.')[-1].lower()
        if file_type == 'txt':
            return file.read().decode('utf-8', errors='ignore')
        elif file_type == 'pdf':
            reader = PyPDF2.PdfReader(file)
            return '\n'.join(page.extract_text() or '' for page in reader.pages)
        elif file_type == 'docx':
            doc = Document(file)
            return '\n'.join(paragraph.text for paragraph in doc.paragraphs)
        return ''

    for file in uploaded_files:
        if file.name.endswith('.zip'):
            with zipfile.ZipFile(file, 'r') as z:
                for inner_file_name in z.namelist():
                    if inner_file_name.endswith(('.txt', '.pdf', '.docx')):
                        with z.open(inner_file_name) as inner_file:
                            content = extract_text(inner_file)
                            st.session_state.detector.add_document(inner_file_name, content)
                            st.session_state.documents.append({
                                'title': inner_file_name,
                                'content': content,
                                'word_count': len(content.split())
                            })
        else:
            content = extract_text(file)
            st.session_state.detector.add_document(file.name, content)
            st.session_state.documents.append({
                'title': file.name,
                'content': content,
                'word_count': len(content.split())
            })

    st.sidebar.success("Fichiers import√©s avec succ√®s!")
    st.session_state.analysis_done = False

# Bouton de suppression des documents
if st.sidebar.button("üóëÔ∏è Supprimer tous les documents"):
    st.session_state.detector = PlagiarismDetector(threshold/100)
    st.session_state.documents = []
    st.session_state.analysis_done = False
    st.sidebar.success("Documents supprim√©s!")

# Interface principale
col1, col2 = st.columns([1, 1])

with col1:
    st.header("üìã Documents Charg√©s")

    if st.session_state.documents:
        to_remove = None

        for i, doc in enumerate(st.session_state.documents):
            col1_doc, col2_doc = st.columns([0.9, 0.1])
            with col1_doc:
                with st.expander(f"üìÑ {doc['title']} ({doc['word_count']} mots)"):
                    st.text_area("Contenu", value=doc['content'][:500], height=100, disabled=True, key=f"doc_content_{i}")
            with col2_doc:
                if st.button("‚ùå", key=f"remove_{i}"):
                    to_remove = i

        # Supprimer le document imm√©diatement
        if to_remove is not None:
            del st.session_state.documents[to_remove]
            del st.session_state.detector.documents[to_remove]
            st.session_state.analysis_done = False
            st.success("Document supprim√© avec succ√®s.")
            st.rerun()

        # Bouton d'analyse
        if st.button("üîç Analyser les Documents", type="primary", use_container_width=True):
            if len(st.session_state.documents) < 2:
                st.error("Au moins 2 documents sont n√©cessaires pour l'analyse")
            else:
                with st.spinner("Analyse en cours..."):
                    try:
                        model = st.session_state.get("selected_model", "Cosine Similarity (TF-IDF)")
                        
                        if model == "Cosine Similarity (TF-IDF)":
                            results = st.session_state.detector.detect_plagiarism()
                            
                        elif model == "SBERT":
                            results = detect_plagiarism_sbert(
                                documents=st.session_state.detector.documents,
                                threshold=st.session_state.detector.threshold
                            )
                            st.session_state.detector.similarities = results
                            
                        elif model == "LSTM":
                            max_len = st.session_state.get("max_sequence_length", 100)
                            results = detect_plagiarism_lstm(
                                documents=st.session_state.detector.documents,
                                model=st.session_state.lstm_model,
                                tokenizer=st.session_state.tokenizer,
                                threshold=st.session_state.detector.threshold,
                                max_length=max_len
                            )
                            st.session_state.detector.similarities = results
                            
                        elif model == "SVM":
                            results = detect_plagiarism_svm(
                                documents=st.session_state.detector.documents,
                                model=st.session_state.svm_model,
                                scaler=st.session_state.scaler,
                                threshold=st.session_state.detector.threshold
                            )
                            st.session_state.detector.similarities = results

                        st.session_state.analysis_done = True
                        if results:
                            st.success(f"Analyse termin√©e avec {model}!")
                        else:
                            st.warning("Aucun r√©sultat obtenu. V√©rifiez vos mod√®les et donn√©es.")
                            
                    except Exception as e:
                        st.error(f"Erreur lors de l'analyse: {str(e)}")
                        st.error("D√©tails de l'erreur pour le d√©bogage:")
                        st.code(str(e))
    else:
        st.info("Aucun document charg√©. Utilisez la barre lat√©rale pour ajouter des documents.")

with col2:
    st.header("üìä R√©sultats de l'Analyse")

    if st.session_state.analysis_done and st.session_state.detector.similarities:
        # Afficher le mod√®le utilis√©
        current_model = st.session_state.get("selected_model", "Cosine Similarity (TF-IDF)")
        st.info(f"R√©sultats obtenus avec: **{current_model}**")
        
        # Tableau des r√©sultats
        df_results = st.session_state.detector.get_detailed_results()
        st.subheader("R√©sultats D√©taill√©s")

        # Formatage du DataFrame pour l'affichage
        df_display = df_results.copy()
        df_display['similarity_percentage'] = df_display['similarity_percentage'].round(2)
        df_display['is_plagiarism'] = df_display['is_plagiarism'].map({True: '‚ö†Ô∏è Plagiat', False: '‚úÖ Original'})
        df_display.columns = ['Document 1', 'Document 2', 'Similarit√© (%)', 'Statut']

        st.dataframe(df_display, use_container_width=True)

        # Statistiques rapides
        plagiarism_count = sum(1 for s in st.session_state.detector.similarities if s['is_plagiarism'])
        total_comparisons = len(st.session_state.detector.similarities)

        col_stat1, col_stat2, col_stat3 = st.columns(3)
        with col_stat1:
            st.metric("Documents", len(st.session_state.documents))
        with col_stat2:
            st.metric("Comparaisons", total_comparisons)
        with col_stat3:
            st.metric("Cas de plagiat", plagiarism_count)

        # Graphique des similarit√©s
        st.subheader("Visualisation des Similarit√©s")

        similarities_data = []
        for s in st.session_state.detector.similarities:
            comparison = f"{s['doc1_title'][:15]}... vs {s['doc2_title'][:15]}..."
            similarities_data.append({
                'Comparaison': comparison,
                'Similarit√© (%)': s['percentage'],
                'Type': 'Plagiat D√©tect√©' if s['is_plagiarism'] else 'Document Original'
            })

        df_viz = pd.DataFrame(similarities_data)

        fig = px.bar(df_viz,
                    x='Comparaison',
                    y='Similarit√© (%)',
                    color='Type',
                    color_discrete_map={
                        'Plagiat D√©tect√©': '#FF6B6B',
                        'Document Original': '#4ECDC4'
                    },
                    title=f'Scores de Similarit√© par Paire de Documents ({current_model})')

        # Ligne de seuil
        fig.add_hline(y=threshold,
                     line_dash="dash",
                     line_color="red",
                     annotation_text=f"Seuil de plagiat ({threshold}%)")

        fig.update_layout(xaxis_tickangle=-45, height=500)
        st.plotly_chart(fig, use_container_width=True)

        # Matrice de similarit√© (seulement pour TF-IDF)
        if current_model == "Cosine Similarity (TF-IDF)" and len(st.session_state.documents) <= 10:
            st.subheader("Matrice de Similarit√©")

            # Calcul de la matrice de similarit√© compl√®te
            similarity_matrix = cosine_similarity(st.session_state.detector.tfidf_matrix)

            # Cr√©ation des labels
            labels = [doc['title'][:20] + '...' if len(doc['title']) > 20
                     else doc['title'] for doc in st.session_state.documents]

            # Heatmap avec Plotly
            fig_heatmap = px.imshow(similarity_matrix,
                                   x=labels,
                                   y=labels,
                                   color_continuous_scale='RdYlBu_r',
                                   aspect='auto',
                                   title='Matrice de Similarit√© entre Documents')

            fig_heatmap.update_layout(height=500)
            st.plotly_chart(fig_heatmap, use_container_width=True)

    elif st.session_state.documents and not st.session_state.analysis_done:
        st.info("Cliquez sur 'Analyser les Documents' pour voir les r√©sultats.")
    else:
        st.info("Aucune analyse disponible. Ajoutez des documents et lancez l'analyse.")

# Section d'aide
with st.expander("‚ÑπÔ∏è Comment utiliser ce d√©tecteur de plagiat"):
    st.markdown("""
    ### Instructions d'utilisation:
    
    1. **Ajouter des documents**: Utilisez la barre lat√©rale pour ajouter vos documents un par un
    2. **Ou charger des fichiers**: Importez des fichiers TXT, PDF, DOCX ou ZIP
    3. **Choisir le mod√®le**: S√©lectionnez le mod√®le de similarit√© (TF-IDF, SBERT, LSTM, SVM)
    4. **Configurer le seuil**: Ajustez le seuil de d√©tection selon vos besoins (70% par d√©faut)
    5. **Analyser**: Cliquez sur "Analyser les Documents" pour d√©tecter les similarit√©s
    6. **Interpr√©ter les r√©sultats**: 
       - Rouge = Plagiat d√©tect√© (au-dessus du seuil)
       - Vert = Document original (en-dessous du seuil)
    
    ### Mod√®les disponibles:
    
    #### 1. **Cosine Similarity (TF-IDF)**
    - M√©thode classique bas√©e sur la fr√©quence des termes
    - Rapide et efficace pour la plupart des cas
    - Id√©al pour d√©tecter les similarit√©s lexicales directes
    
    #### 2. **SBERT (Sentence-BERT)**
    - Mod√®le de langue pr√©-entra√Æn√©
    - Comprend le sens s√©mantique des phrases
    - Excellent pour d√©tecter les paraphrases
    
    #### 3. **LSTM (Long Short-Term Memory)**
    - R√©seau de neurones r√©currents
    - Analyse les s√©quences de mots
    - Bon pour capturer les d√©pendances √† long terme
    
    #### 4. **SVM (Support Vector Machine)**
    - Mod√®le d'apprentissage supervis√©
    - Utilise des features avanc√©es bas√©es sur SBERT
    - Tr√®s pr√©cis avec un entra√Ænement appropri√©
    
    ### Param√®tres sp√©ciaux:
    - **LSTM**: Vous pouvez ajuster la longueur maximale des s√©quences
    - **Tous les mod√®les**: Le seuil de d√©tection est personnalisable
    
    ### Formats de fichiers support√©s:
    - **TXT**: Fichiers texte brut
    - **PDF**: Documents PDF (extraction automatique du texte)
    - **DOCX**: Documents Word
    - **ZIP**: Archives contenant plusieurs fichiers
    
    ### Limites et consid√©rations:
    - **TF-IDF**: Sensible √† la similarit√© lexicale, moins efficace pour les paraphrases
    - **SBERT**: N√©cessite une connexion internet pour le premier t√©l√©chargement
    - **LSTM**: D√©pend de la qualit√© du mod√®le pr√©-entra√Æn√©
    - **SVM**: N√©cessite les fichiers de mod√®le (svm_model2.joblib, scaler2.joblib)
    - **Performance**: Les mod√®les deep learning (SBERT, LSTM, SVM) sont plus lents mais plus pr√©cis
    
    ### Conseils d'utilisation:
    - Utilisez **TF-IDF** pour une analyse rapide de similarit√© lexicale
    - Utilisez **SBERT** pour d√©tecter les paraphrases et reformulations
    - Utilisez **LSTM** pour analyser les structures s√©quentielles
    - Utilisez **SVM** pour la plus haute pr√©cision (si le mod√®le est bien entra√Æn√©)
    - Ajustez le seuil selon votre contexte (plus strict = moins de faux positifs)
    
    ### Interpr√©tation des r√©sultats:
    - **Scores √©lev√©s (>80%)**: Plagiat tr√®s probable
    - **Scores moyens (50-80%)**: Similarit√© significative, √† examiner
    - **Scores faibles (<50%)**: Documents probablement originaux
    - **Matrice de similarit√©**: Vue d'ensemble des relations entre tous les documents
    """)

# Informations sur les mod√®les charg√©s
st.sidebar.header("üìä √âtat des Mod√®les")
with st.sidebar.expander("Mod√®les Disponibles", expanded=False):
    # V√©rification TF-IDF
    st.write("‚úÖ **TF-IDF**: Toujours disponible")
    
    # V√©rification SBERT
    if st.session_state.sbert_model is not None:
        st.write("‚úÖ **SBERT**: Mod√®le charg√©")
    else:
        st.write("‚ùå **SBERT**: Non disponible")
    
    # V√©rification LSTM
    if st.session_state.lstm_model is not None and st.session_state.tokenizer is not None:
        st.write("‚úÖ **LSTM**: Mod√®le et tokenizer charg√©s")
        st.write(f"   - Mod√®le: lstm_model2.h5")
        st.write(f"   - Tokenizer: tokenizer2.joblib")
    else:
        st.write("‚ùå **LSTM**: Mod√®le ou tokenizer manquant")
        if st.session_state.lstm_model is None:
            st.write("   - ‚ùå lstm_model2.h5 non trouv√©")
        if st.session_state.tokenizer is None:
            st.write("   - ‚ùå tokenizer2.joblib non trouv√©")
    
    # V√©rification SVM
    if st.session_state.svm_model is not None and st.session_state.scaler is not None:
        st.write("‚úÖ **SVM**: Mod√®le et scaler charg√©s")
        st.write(f"   - Mod√®le: svm_model2.joblib")
        st.write(f"   - Scaler: scaler2.joblib")
    else:
        st.write("‚ùå **SVM**: Mod√®le ou scaler manquant")
        if st.session_state.svm_model is None:
            st.write("   - ‚ùå svm_model2.joblib non trouv√©")
        if st.session_state.scaler is None:
            st.write("   - ‚ùå scaler2.joblib non trouv√©")

# Footer avec informations techniques
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #7F8C8D;'>
    <p><strong>D√©tecteur de Plagiat IA Multi-Mod√®les</strong></p>
    <p>Supportant TF-IDF, SBERT, LSTM et SVM ‚Ä¢ D√©velopp√© avec Streamlit et TensorFlow/Scikit-learn</p>
    <p>Pour de meilleurs r√©sultats, assurez-vous que tous les fichiers de mod√®les sont pr√©sents dans le dossier de l'application</p>
</div>
""", unsafe_allow_html=True)
